{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch] accelerate -U\n",
        "!pip install openpyxl scikit-learn umap-learn\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import umap\n",
        "import numpy as np\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "from openpyxl import Workbook, load_workbook\n",
        "from openpyxl.styles import Font, Alignment\n",
        "import logging\n",
        "\n",
        "# Configuration - the file paths will need to be changed when someone else run's the code\n",
        "FILE_PATH = '/content/Original Hourly Billing Activity Details.xlsx'\n",
        "OUTPUT_DIR = '/content/'\n",
        "\n",
        "# Logging setup\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "logging.info(f\"Using device: {device}\")\n",
        "\n",
        "# Function to load data\n",
        "def load_data(file_path):\n",
        "    try:\n",
        "        data = pd.read_excel(file_path, sheet_name='Export')\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading data: {e}\")\n",
        "        raise\n",
        "\n",
        "# Function to preprocess data\n",
        "def preprocess_data(data):\n",
        "    data['Hours(Duration)'] = data.apply(\n",
        "        lambda row: row['Billable Amount'] / row['Hourly Rate'] if pd.isnull(row['Hours(Duration)']) else row['Hours(Duration)'],\n",
        "        axis=1\n",
        "    )\n",
        "    data['Text'] = data['Service Type'].astype(str) + \" \" + data['Description'].astype(str)\n",
        "    data['Text'] = data['Text'].fillna('').astype(str)\n",
        "    data = data[~data['Text'].str.contains('nan nan')]\n",
        "    return data\n",
        "\n",
        "# Function to generate BERT embeddings\n",
        "def generate_embeddings(texts, tokenizer, model, device, batch_size=32):\n",
        "    embeddings = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i+batch_size].tolist()\n",
        "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "        embeddings.append(batch_embeddings)\n",
        "    embeddings = np.vstack(embeddings)\n",
        "    return embeddings\n",
        "\n",
        "# Function to perform clustering\n",
        "def perform_clustering(features, num_clusters=50):\n",
        "    scaler = StandardScaler()\n",
        "    features_scaled = scaler.fit_transform(features)\n",
        "    umap_model = umap.UMAP(n_components=10, random_state=42)\n",
        "    features_reduced = umap_model.fit_transform(features_scaled)\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "    clusters = kmeans.fit_predict(features_reduced)\n",
        "    return clusters\n",
        "\n",
        "# Function to map categories and rank automation ease\n",
        "def map_categories_and_rank(data):\n",
        "    category_names = {\n",
        "        0: \"Bank Feeds Processing\",\n",
        "        1: \"Expense Reimbursement\",\n",
        "        2: \"Accounts Payable\",\n",
        "        3: \"Client Communication\",\n",
        "        4: \"Invoice Processing\",\n",
        "        5: \"Financial Reporting\",\n",
        "        6: \"Payroll Processing and Reconciliation\",\n",
        "        7: \"Vendor Management\",\n",
        "        8: \"Month-End Close\",\n",
        "        9: \"General Administration\",\n",
        "        10: \"Audit Support and Compliance\",\n",
        "        11: \"AR Management\",\n",
        "        12: \"Employee Benefits Administration\",\n",
        "        13: \"Budgeting and Forecasting\",\n",
        "        14: \"Cash Management and Flow\",\n",
        "        15: \"Tax Preparation and Filings\",\n",
        "        16: \"Data Entry\",\n",
        "        17: \"Compliance\",\n",
        "        18: \"Training and Onboarding\",\n",
        "        19: \"Strategic Advisory\",\n",
        "        20: \"Financial Analysis\",\n",
        "        21: \"Financial Modeling and Analysis\",\n",
        "        22: \"Operational Management\",\n",
        "        23: \"Consulting\",\n",
        "        24: \"Tax Advisory\",\n",
        "        25: \"Financial Planning\",\n",
        "        26: \"Regulatory Reporting\",\n",
        "        27: \"Investment Management\",\n",
        "        28: \"Project Management\",\n",
        "        29: \"Risk Management and Assessment\",\n",
        "        30: \"Client Day-to-Day Touchpoint\",\n",
        "        31: \"Client Communication and Correspondence\",\n",
        "        32: \"Vendor Bill Processing and Communication\",\n",
        "        33: \"AR Deposits and Review\",\n",
        "        34: \"Expense Entries and Reconciliation\",\n",
        "        35: \"General Ledger Review\",\n",
        "        36: \"Financial Statements Preparation\",\n",
        "        37: \"Audit and Compliance Support\",\n",
        "        38: \"Training and Support for New Employees\",\n",
        "        39: \"Monthly and Quarterly Close\",\n",
        "        40: \"Revenue Recognition\",\n",
        "        41: \"Budget Preparation and Review\",\n",
        "        42: \"Cash Flow Management\",\n",
        "        43: \"Employee Benefits Administration\",\n",
        "        44: \"Financial Modeling and Analysis\",\n",
        "        45: \"Investment Advisory\",\n",
        "        46: \"Risk Assessment and Management\",\n",
        "        47: \"Strategic Financial Planning\"\n",
        "    }\n",
        "\n",
        "    data['Category'] = data['Cluster'].map(category_names)\n",
        "\n",
        "    # Define automation ease\n",
        "    automation_ease = {\n",
        "        \"Bank Feeds Processing\": 5,\n",
        "        \"Accounts Payable\": 5,\n",
        "        \"Invoice Processing\": 5,\n",
        "        \"Cash Management\": 5,\n",
        "        \"Tax Preparation\": 5,\n",
        "        \"Data Entry\": 5,\n",
        "        \"Expense Reimbursement\": 4,\n",
        "        \"Payroll Processing\": 4,\n",
        "        \"Vendor Management\": 4,\n",
        "        \"General Administration\": 4,\n",
        "        \"AR Management\": 4,\n",
        "        \"Employee Benefits\": 4,\n",
        "        \"Compliance\": 4,\n",
        "        \"Financial Modeling\": 4,\n",
        "        \"Operational Management\": 4,\n",
        "        \"Tax Advisory\": 4,\n",
        "        \"Financial Planning\": 4,\n",
        "        \"Regulatory Reporting\": 4,\n",
        "        \"Investment Management\": 4,\n",
        "        \"Project Management\": 4,\n",
        "        \"Risk Management\": 4,\n",
        "        \"Client Communication\": 2,\n",
        "        \"Financial Reporting\": 2,\n",
        "        \"Month-End Close\": 2,\n",
        "        \"Audit Support\": 2,\n",
        "        \"Budgeting and Forecasting\": 2,\n",
        "        \"Training and Onboarding\": 2,\n",
        "        \"Strategic Advisory\": 2,\n",
        "        \"Financial Analysis\": 2,\n",
        "        \"Consulting\": 2\n",
        "    }\n",
        "\n",
        "    data['Automation Ease'] = data['Category'].map(automation_ease).fillna(3)  # Default to 3 for categories not explicitly listed\n",
        "    return data\n",
        "\n",
        "# Main execution flow\n",
        "def main():\n",
        "    # Load data\n",
        "    data = load_data(FILE_PATH)\n",
        "\n",
        "    # Preprocess data\n",
        "    data = preprocess_data(data)\n",
        "    logging.info(f\"Total hours after preprocessing: {data['Hours(Duration)'].sum()}\")\n",
        "\n",
        "    # Load BERT tokenizer and model\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    model = BertModel.from_pretrained('bert-base-uncased')\n",
        "    model.to(device)\n",
        "\n",
        "    # Generate embeddings\n",
        "    embeddings = generate_embeddings(data['Text'], tokenizer, model, device)\n",
        "\n",
        "    # Generate TF-IDF vectors\n",
        "    vectorizer = TfidfVectorizer(max_features=5000)\n",
        "    tfidf_matrix = vectorizer.fit_transform(data['Text'])\n",
        "\n",
        "    # Combine BERT embeddings and TF-IDF vectors\n",
        "    combined_features = np.hstack((embeddings, tfidf_matrix.toarray()))\n",
        "\n",
        "    # Perform clustering\n",
        "    data['Cluster'] = perform_clustering(combined_features)\n",
        "\n",
        "    # Save the clusters to inspect the categories\n",
        "    clustered_data = data[['Client Name', 'Operator', 'Cluster', 'Text', 'Hours(Duration)']]\n",
        "    clustered_data.to_excel(os.path.join(OUTPUT_DIR, 'Clustered_Data.xlsx'), index=False)\n",
        "    logging.info(\"Clustered data saved for inspection.\")\n",
        "\n",
        "    # Check total hours after clustering\n",
        "    logging.info(f\"Total hours after clustering: {data['Hours(Duration)'].sum()}\")\n",
        "\n",
        "    # Map categories and rank automation ease\n",
        "    data = map_categories_and_rank(data)\n",
        "\n",
        "    # Cost Analysis\n",
        "    data['Cost Per Hour'] = data['Billable Amount'] / data['Hours(Duration)']\n",
        "    category_costs = data.groupby('Category')['Billable Amount'].sum().reset_index()\n",
        "    category_costs.rename(columns={'Billable Amount': 'Total Cost'}, inplace=True)\n",
        "    data = pd.merge(data, category_costs, on='Category', how='left')\n",
        "\n",
        "    # Save the initial data for inspection\n",
        "    initial_data_file = os.path.join(OUTPUT_DIR, 'Initial_Data_With_Categories_And_Automation_Ease.xlsx')\n",
        "    data.to_excel(initial_data_file, index=False)\n",
        "    logging.info(f\"Initial data with categories and automation ease saved as {initial_data_file}\")\n",
        "\n",
        "    # Group and summarize data\n",
        "    grouped = data.groupby(['Client Name', 'Operator', 'Category', 'Automation Ease']).agg({'Hours(Duration)': 'sum', 'Billable Amount': 'sum', 'Total Cost': 'first'}).reset_index()\n",
        "    grouped['Cost Per Hour'] = grouped['Billable Amount'] / grouped['Hours(Duration)']\n",
        "    grouped = grouped.sort_values(by=['Client Name', 'Hours(Duration)', 'Automation Ease'], ascending=[True, False, True])\n",
        "\n",
        "    summary = grouped.groupby(['Category', 'Automation Ease']).agg({'Hours(Duration)': 'sum', 'Billable Amount': 'sum', 'Total Cost': 'first'}).reset_index()\n",
        "\n",
        "    # Debug: Check total hours in summary\n",
        "    logging.info(f\"Total hours in summary: {summary['Hours(Duration)'].sum()}\")\n",
        "\n",
        "    # Save the summary data for inspection\n",
        "    summary_file = os.path.join(OUTPUT_DIR, 'Summary_Data_Updated.xlsx')\n",
        "    summary.to_excel(summary_file, index=False)\n",
        "    logging.info(f\"Summary data saved as {summary_file}\")\n",
        "\n",
        "    # Save the data to a single Excel file with multiple sheets\n",
        "    output_file_path = os.path.join(OUTPUT_DIR, 'Hourly_Billing_Activity_Summary_Formatted_Updated.xlsx')\n",
        "    with pd.ExcelWriter(output_file_path, engine='openpyxl') as writer:\n",
        "        # Save initial data with categories and automation ease\n",
        "        data.to_excel(writer, sheet_name='Initial Data', index=False)\n",
        "\n",
        "        # Save grouped data\n",
        "        grouped.to_excel(writer, sheet_name='Grouped Data', index=False)\n",
        "\n",
        "        # Save summary data\n",
        "        summary.to_excel(writer, sheet_name='Summary Data', index=False)\n",
        "\n",
        "        # Adding filters and formatting\n",
        "        for sheet_name in ['Initial Data', 'Grouped Data', 'Summary Data']:\n",
        "            ws = writer.sheets[sheet_name]\n",
        "            for col in ws.columns:\n",
        "                max_length = 0\n",
        "                column = col[0].column_letter  # Get the column name\n",
        "                for cell in col:\n",
        "                    try:\n",
        "                        if len(str(cell.value)) > max_length:\n",
        "                            max_length = len(str(cell.value))\n",
        "                    except:\n",
        "                        pass\n",
        "                adjusted_width = (max_length + 2)\n",
        "                ws.column_dimensions[column].width = adjusted_width\n",
        "\n",
        "            # Add filters to all columns\n",
        "            ws.auto_filter.ref = ws.dimensions\n",
        "\n",
        "            # Center align all columns\n",
        "            for row in ws.iter_rows():\n",
        "                for cell in row:\n",
        "                    cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
        "\n",
        "            # Format the headers\n",
        "            header_font = Font(bold=True)\n",
        "            for cell in ws[1]:\n",
        "                cell.font = header_font\n",
        "\n",
        "        # Add a note explaining the automation ease ranking\n",
        "        ws = writer.sheets['Initial Data']\n",
        "        ws['A1'] = \"Automation Ease Ranking: 5 = Easily Automated, 1 = Not Easily Automated\"\n",
        "        ws['A1'].font = Font(bold=True)\n",
        "\n",
        "    logging.info(f\"Output file saved as {output_file_path}\")\n",
        "\n",
        "    # Debug: Check for any rows in the original data that are not included in the grouping\n",
        "    original_hours = data['Hours(Duration)'].sum()\n",
        "    grouped_hours = grouped['Hours(Duration)'].sum()\n",
        "    if original_hours != grouped_hours:\n",
        "        missing_hours = original_hours - grouped_hours\n",
        "        logging.warning(f\"Missing hours: {missing_hours}\")\n",
        "        missing_data = data[~data.set_index(['Client Name', 'Category']).index.isin(grouped.set_index(['Client Name', 'Category']).index)]\n",
        "        logging.warning(\"Rows in the original data that are not included in the grouping:\")\n",
        "        logging.warning(missing_data)\n",
        "        missing_data.to_excel(os.path.join(OUTPUT_DIR, 'Missing_Data_Updated.xlsx'), index=False)\n",
        "    else:\n",
        "        logging.info(\"All hours are accounted for in the grouping.\")\n",
        "\n",
        "    # Dashboard\n",
        "    # Creating a dashboard using Power BI or Tableau\n",
        "    # (This step requires the use of external tools like Power BI or Tableau. Export the data as CSV to use in these tools.)\n",
        "\n",
        "    # Save data as CSV for use in Power BI or Tableau\n",
        "    data.to_csv(os.path.join(OUTPUT_DIR, 'Initial_Data_With_Categories_And_Automation_Ease.csv'), index=False)\n",
        "    grouped.to_csv(os.path.join(OUTPUT_DIR, 'Grouped_Data.csv'), index=False)\n",
        "    summary.to_csv(os.path.join(OUTPUT_DIR, 'Summary_Data_Updated.csv'), index=False)\n",
        "\n",
        "    logging.info(\"Data saved as CSV for use in Power BI or Tableau.\")\n",
        "    logging.info(\"Script execution completed successfully.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "id": "IHYRwmKqbac7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyO+6KChSAy8SBqfLUKOXUMD"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}